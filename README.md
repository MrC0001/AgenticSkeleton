# AgenticSkeleton

A simple Flask-based AI agent skeleton with both mock responses and Azure OpenAI integration.

## Quick Start

```bash
# Clone repository
git clone https://github.com/MrC0001/AgenticSkeleton.git
cd AgenticSkeleton

# Set up environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac OR .venv\Scripts\activate for Windows
pip install -r requirements.txt
cp .env.example .env

# Start the server
python -m agentic_skeleton

# In another terminal, test the API
python -m agentic_skeleton.misc.simple_primer_test
```

## Project Structure

```
agentic_skeleton/
â”œâ”€â”€ __init__.py         # Package initialization
â”œâ”€â”€ __main__.py         # Main entry point
â”œâ”€â”€ api/                # API endpoints
â”‚   â””â”€â”€ endpoints.py    # Flask routes
â”œâ”€â”€ config/             # Configuration
â”‚   â””â”€â”€ settings.py     # Application settings
â”œâ”€â”€ core/               # Core functionality
â”‚   â”œâ”€â”€ azure_integration.py  # Azure OpenAI integration
â”‚   â””â”€â”€ mock_responses.py     # Mock response generation
â”œâ”€â”€ misc/               # Miscellaneous utilities
â”‚   â”œâ”€â”€ simple_primer.py      # Simplified version of the app
â”‚   â””â”€â”€ simple_primer_test.py # Test client for the simplified app
â”œâ”€â”€ tests/              # Test suite
â”‚   â”œâ”€â”€ test_api.py     # API tests
â”‚   â”œâ”€â”€ test_integration.py  # Integration tests
â”‚   â””â”€â”€ test_unit.py    # Unit tests
â””â”€â”€ utils/              # Utility functions
    â””â”€â”€ helpers.py      # Helper functions
```

## Configuration

By default, the application runs in mock mode which requires no API keys. To switch to Azure OpenAI:

1. Edit your `.env` file:
   ```
   # Change this to false for Azure mode
   MOCK_RESPONSES=false
   
   # Add your Azure OpenAI credentials
   AZURE_OPENAI_KEY=your-key-here
   AZURE_OPENAI_ENDPOINT=your-endpoint-here
   ```

2. Make sure the `openai` dependency is installed:
   ```
   pip install openai
   ```

## Usage

### Mock Mode (Default)

By default, the application runs in mock mode, which doesn't require any Azure credentials.

```bash
# Ensure MOCK_RESPONSES=true in .env file
python -m agentic_skeleton
```

### Azure OpenAI Mode

To use with Azure OpenAI:

1. Set `MOCK_RESPONSES=false` in your `.env` file
2. Add your Azure OpenAI credentials:
   - `AZURE_OPENAI_KEY`
   - `AZURE_OPENAI_ENDPOINT`
3. Run the application

## Simplified Version

The project includes a simplified version in the `agentic_skeleton/misc/` directory:

- `simple_primer.py`: A streamlined version of the main application
- `simple_primer_test.py`: A test client for the simplified app

To run the simplified version:

```bash
# Start the server
python -m agentic_skeleton.misc.simple_primer

# In another terminal, test the API
python -m agentic_skeleton.misc.simple_primer_test
```

## API Usage

### Health Check
```
GET /health
```
Response:
```json
{
  "status": "healthy",
  "mode": "mock",  // or "azure" in production mode
  "version": "1.0.0"
}
```

### Run Agent
```
POST /run-agent
Body: {"request": "your user request here"}
```

Response:
```json
{
  "plan": [
    "Research the topic thoroughly",
    "Create an outline",
    "Write the first draft",
    "Revise and edit the content",
    "Format according to requirements"
  ],
  "results": [
    {
      "subtask": "Research the topic thoroughly",
      "result": "Found multiple sources confirming that thoroughly has significant implications."
    },
    // Additional results...
  ]
}
```

## Testing

The project includes three different testing approaches, all with enhanced output formatting:

### 1. Basic API Testing

A simple test script demonstrates how to use the API:

```bash
# Start the server in one terminal
python -m agentic_skeleton

# Run the test in another terminal
python -m agentic_skeleton.misc.simple_primer_test
```

Example output:
```
=============== Testing Agentic Skeleton API ===============
âœ… Server is healthy (Mode: mock)
ğŸ”„ Testing request: "Write a short blog post about AI agents"
  â”œâ”€ Sending request to http://localhost:8000/run-agent
  â”œâ”€ Response time: 79ms
  â””â”€ Plan received with 5 steps
    
ğŸ“Š Response Summary:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Subtask                         â”‚ Result                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Research the topic thoroughly   â”‚ Found multiple sources confirming   â”‚
â”‚                                 â”‚ that ai has significant ...         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Create an outline               â”‚ Here's a concise summary of         â”‚
â”‚                                 â”‚ outline: It represents an ...       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ... more results ...            â”‚ ...                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Unit Tests

Comprehensive unit tests validate the core functionality:

```bash
python -m test_unit
```

Example output:
```
=============== Running Unit Tests ===============
âœ… test_health_check_endpoint - PASS (12ms)
âœ… test_agent_endpoint_mock_mode - PASS (18ms)
âœ… test_task_response_generation - PASS (5ms)
âœ… test_error_handling - PASS (15ms)
âœ… test_topic_extraction - PASS (3ms)

ğŸ“Š Test Summary:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test Category                  â”‚ Status â”‚ Time  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ API Endpoints                  â”‚ PASS   â”‚ 30ms  â”‚
â”‚ Response Generation            â”‚ PASS   â”‚ 8ms   â”‚
â”‚ Error Handling                 â”‚ PASS   â”‚ 15ms  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL                          â”‚ 5/5    â”‚ 53ms  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Integration Tests

Integration tests validate the full request-response cycle:

```bash
python -m test_integration
```

Example output:
```
=============== Running Integration Tests ===============
ğŸš€ Starting test server on port 8001
â³ Waiting for server to start...
âœ… Server is running

â–¶ï¸ Running integration tests:
âœ… test_basic_request - PASS (132ms)
âœ… test_analytical_request - PASS (118ms)
âœ… test_creative_request - PASS (125ms)
âœ… test_technical_request - PASS (142ms)
âœ… test_error_handling - PASS (28ms)

ğŸ“ˆ Performance Metrics:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Request Type        â”‚ Status    â”‚ Response Time â”‚ Plan Size â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Basic               â”‚ âœ… PASS   â”‚ 132ms         â”‚ 5 steps   â”‚
â”‚ Analytical          â”‚ âœ… PASS   â”‚ 118ms         â”‚ 6 steps   â”‚
â”‚ Creative            â”‚ âœ… PASS   â”‚ 125ms         â”‚ 5 steps   â”‚
â”‚ Technical           â”‚ âœ… PASS   â”‚ 142ms         â”‚ 7 steps   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AVERAGE             â”‚ 100%      â”‚ 129ms         â”‚ 5.8 steps â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ›‘ Stopping test server
```

## Enhanced Test Output

All test scripts include enhanced output formatting with:

- âœ… Colorized test results with pass/fail indicators
- â±ï¸ Detailed timing metrics for performance analysis
- ğŸ“Š Performance comparison tables with response times
- ğŸ”„ Visual progress indicators for long-running tests
- ğŸ“‹ JSON response visualization with syntax highlighting
- ğŸ“ˆ Summary tables with success rates and statistics

For the best experience, ensure you have the termcolor package installed:

```bash
pip install termcolor
```

## Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `MOCK_RESPONSES` | Use mock responses instead of Azure | `true` |
| `AZURE_OPENAI_KEY` | Azure OpenAI API key | N/A |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI endpoint URL | N/A |
| `MODEL_PLANNER` | Model for planning tasks | `o3-mini` |
| `MODEL_EXECUTOR` | Model for execution tasks | `claude_3.7_sonnet` |
| `PORT` | Server port | `8000` |

## Dependencies

- Flask: Web framework for the API server
- python-dotenv: Environment variable management
- requests: HTTP client for API calls
- termcolor: For colored test output
- openai: For Azure OpenAI integration (optional)
- gunicorn: For production deployment (optional)

## Development

### Running Tests

```bash
# Run all tests
python -m test_unit && python -m test_integration

# Run a specific test file
python -m test_unit
python -m test_integration

# Run the basic test script (server must be running)
python -m agentic_skeleton.misc.simple_primer_test
```

### Adding New Test Cases

To add new test cases:

1. For unit tests: Add new test methods to `test_unit.py`
2. For integration tests: Add new test methods to `test_integration.py`

Example of adding a new test case:

```python
def test_new_feature(self):
    """Test a new feature of the API"""
    # Setup test data
    test_data = {"request": "Test the new feature"}
    
    # Make request
    response = requests.post(f"{self.base_url}/run-agent", json=test_data)
    
    # Assert expectations
    self.assertEqual(response.status_code, 200)
    data = response.json()
    self.assertIn("plan", data)
    self.assertIn("results", data)
    
    # Additional assertions specific to this test case
    # ...
```

